---
title: "qbs121_hw2_gibran"
author: "Gibran Erlangga"
date: "1/16/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problems (Bonus)
## 2. Show that minimizing mean square error is the same as maximzing $R^2$ is the same as minimizing the sum of squares.

## 4. Suppose you add to your model the interactions of two categorical variables, and that the number of categories of these two categorical variables are r and s respectively. How many degrees of freedom are used by the interaction?

## 6. a. Suppose E[log(Y)|X1, X2] = b0 + b1 log(X1) + b2X2. How does a $k$ fold increase in X1
affect the expected value of Y holding X2 constant?

# Data Analyses
## 2.1 Analysis of the FEV Data
Load the data.
``` {r}
FEV.Data <- read.delim("http://jse.amstat.org/datasets/fev.dat.txt", sep="", header=FALSE)
names(FEV.Data) <- c("Age","FEV","Height","Male","Smoker")
attach(FEV.Data)
```
1. Effect of Smoking: Report the effect of smoking on FEV, using a univariable model (unadjusted) and multivariable model adjusting for age, height and gender.

``` {r}
summary(lm(FEV ~ Smoker))

summary(lm(FEV ~ Smoker + Age + Height + Male))
```

2. Effect of Age and Gender: Test if the effect of age on FEV is different in males and females. If so, do subgroup analyses reporting the effect of age in males and females separately.
``` {r}
# overall
summary(lm(FEV ~ Age + Male))

# female
summary(lm(FEV ~ Age, subset=Male==0))

# male
summary(lm(FEV ~ Age, subset=Male==1))
```

3. Effect of Height and Gender: Test if the effect of height on FEV is different in males and females. If so, do subgroup analyses reporting the effect of height in males and females separately.
``` {r}
# overall
summary(lm(FEV ~ Height + Male))

# female
summary(lm(FEV ~ Height, subset=Male==0))

# male
summary(lm(FEV ~ Height, subset=Male==1))
```

## 2.1 Analysis of HSB Data
Download the following dataset and install the library multcomp.
``` {r}
hsb2 <- read.csv("https://stats.idre.ucla.edu/stat/data/hsb2.csv")
library(multcomp)
```

1. Model ”read” in terms of female, schtyp and ses (as a factor); 
``` {r}
model <- lm(read ~ female + schtyp + factor(ses), hsb2)
summary(model)
```

2. Show the first few rows of the design matrix.
``` {r}
head(X <- model.matrix(~female + schtyp + factor(ses), hsb2), 10)
```

3. Calculate formula where Y is the reading score and X is the design matrix.
``` {r}
Y <- hsb2$read

solve(t(X) %*% X) %*% t(X) %*% Y
```

4. Compare the value computed in the previous step to the coefficients from the lm. Are they the same?

Yes, they are close enough.
``` {r}
summary(model$coefficients)
```

5. Use the ”waldtest” function of the library ”lmtest” to test the null hypothesis that the factor ses explains no variation in reading scores.
``` {r}
library(lmtest)

model_null <- lm(read ~ female + schtyp, hsb2)
waldtest(model_null, model)
```

6. Repeat the last step manually using syntax like t(coef(o)[4:5]) %*% solve(vcov(o))[4:5] %*%
coef(o)[4:5] to create the test statistic and using an F-test.
``` {r}
t_statistic <- t(coef(model)[4:5]) %*% solve(vcov(model))[4:5] %*% coef(model)[4:5]

p_val <- 1 - pf(t_statistic, df1=2, df2=model$df.residual)
p_val
```

## 2.3 Smoothing
1. Locate the dataset ”ryegrass” in the CRAN library ”drc”.
``` {r}
library(drc)
data <- ryegrass
attach(data)
```
2. Fit a straightline to the data and superimpose it on the scatterplot of rootl versus conc.
``` {r}
plot(rootl, conc, data=data)
summary(o <- lm(rootl ~ conc, data=data))
abline(o)
```

3. Using different colors add a quadratic fit of rootl versus conc.
``` {r}
#create a new variable for conc2
data$conc2 <- data$conc^2

#fit quadratic regression model
quadraticModel <- lm(rootl ~ conc + conc2, data=data)

#view model summary
summary(quadraticModel)

plot(rootl, conc, data=data)
summary(o <- lm(rootl ~ conc, data=data))
summary(q <- quadraticModel)
abline(o, col=c("red"))
abline(q, col=c("blue"))
```

4. Use the gam function from the gam library to fit a smooth curve.
``` {r}
library(gam)
library(mgcv)

gam_model <- gam(rootl ~ s(conc, k=7), data=data)

xvals <- data.frame(seq(0, 30, 0.1))
colnames(xvals) <- "conc"

gam_pred <- predict.gam(gam_model, xvals)

plot(rootl, conc, data=data)
lines(xvals$conc, gam_pred)

```


## 2.5 Simulate and Analyze 2
1. Generate the following data consisting of a dependent variable Y an exposure of interest X and a covariate Z.
``` {r}
set.seed(121)

n <- 300
Z <- runif(n) < 0.5
X <- rnorm(n) + ifelse(Z, 1.5, -1.5) 
Y<- ifelse(Z, X-2.5, X+2.5) + rnorm(n) 
summary(lm(Y~X)) $coef
```

2. Interpret the results of the linear regression, and conclude if Y increases, decreases or has no association with X.

Based on the linear regression result above, for every one unit increase of X, the value of Y decreases by 0.14 point.

3. Now consider the covariate Z. Is it associated with Y?
``` {r}
summary(lm(Y ~ Z))$coef
```


4. Run and interpret the following analyses.
``` {r}
summary(lm(Y~X, subset=Z))$coef 
summary(lm(Y~X, subset=!Z))$coef
summary(lm(Y~X + Z))$coef
plot(X, Y, col=ifelse (Z,2 ,5))
```

5. Comment on the disparity in results for the association of Y and X.
Based on the regression results above, Z has a significant effect on Y given the p-value score shown above.

6. Test if there is an interaction of X and Z.
``` {r}
summary(lm(Y ~ X*Z))$coef
```

