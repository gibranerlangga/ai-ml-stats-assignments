
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[doublespacing]{setspace}
\usepackage{geometry}
\usepackage{listings}

\newenvironment{enum1}
{\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}


\setcounter{MaxMatrixCols}{10}

\geometry{left=1in,right=1in,top=1in,bottom=1in}

\begin{document}

\title{QBS 121: Assignment on GLM and Modeling Counts}

\maketitle

\pagestyle{headings}

{\bf Do the following questions: 1.1, 1.2, 2.1, 3.1, 3.2}

\section{Data Analyses}

\subsection{Modelling Student Absences}

Analyze the dataset {\it quine} which comes with the R library {\it MASS}. The dependent variable is number of student absences. 
\begin{enum1}
\item Put together a table of univariable (1 covariate at at time) results on how each of the covariates relate to student absences.
\item Put together a table of multivariable results, i.e., run a multivariable model using all of the variables (or a subset if you choose).
\item Do this in two ways, (i) using Poisson regression in conjunction with sandwich variance to determine standard errors (or by selecting family=quasipoisson in the glm function) and (ii) negative binomial regression. Comment on the difference or similarity between the two sets of results.
\end{enum1}

\subsection{Cancer Counts in Danish Cities}

Access the data {\it eba1977} in the R library ISwR. This is a small dataset on cancer counts by city and age group in Denmark.
\begin{enum1}
\item Which variable makes sense to use as an offset.
\item a. Use Poisson regression to model the association with age group. b. Test the significance of age using anova(o.glm, test="Chisq") c. Test the association of age as an ordinal variable (hint: create ageOrdinal = as.numeric(age)).
\item a. Use Poisson regression to model the association with city. b. Test the significance of city. 
\item Run the multivariable model with city and age.
\item For interest, instead of using an offset include log(population) as a covariate. Is the coefficient significantly different from 1.0 ?
\end{enum1}

\subsection{Modelling Incidence of Hypoglycemia in Children with Type 1 Diabetes}

The event of interest (dependent variable) is hypoglycemia, an episode in which an individual needs assistance because their glucose level is too low usually because they had taken too much insulin. The dependent variable comes in two piecies of information, (i) the number of hypoglycemic events and (ii) the follow-up time.
\begin{enum1}
\item Estimate the incidence of hypoglycemia in this cohort as the total number of hypoglycemia divided by total follow-up time.
\item Run a Poisson model for the incidence of hypoglycemia (account for variable follow-up time using an offset or adding log of follow-up time as a covariate) without  covariates.
\item Run univariable analyses for each of the covariates.  Account for variable follow-up time using an offset or adding log of follow-up time as a covariate. Account for over (or under dispersion) by employing a sandwich correction or specifying family=quassipoisson within glm.
\item Repeat this using negative binomial (gamma-Poisson) regression.
\item Develop a multivariable model using either LASSO, forwards or backwards stepwise regression.
\item Using the results of the multivariable model comment on the effect of insurance status on hypoglycemic incidence.
\end{enum1}


\section{Simulate and Analyze}

\subsection{Large Counts: Linear Regression vs Poisson}

If the dependent variable is a count that takes large values (e.g. counts that are zero with very low frequency) it may be preferable to use linear regression.

\begin{enum1}
	\item Choose a sample size, e.g. n=500
	\item Generate a couple continuous variables, Z1=rnorm(n) and Z2=rnorm(n)
	\item Generate a large count Y=rpois(n, lambda=100*1.5**Z1/1.2**Z2)
	\item Plot this count vs Z1, and then versus Z2
	\item Use multivariable Poisson regression to model Y vs Z1 and Z2.
	\item Use multivariable linear regression to model Y vz Z1 and Z2
	\item Use multivariable linear regression to model log(Y) vz Z1 and Z2
	\item Assess similarities and differences of the estimates, standard errors and Z-values from these three models
\end{enum1}

\subsection{Binary Endpoint: Logistic vs Poisson}

Sometime Poisson regression is used when the dependent variable is binary in order to get a coefficient which can be interpreted as a log risk ratio, as opposed to logistic regression which yields a log odds ratio.

\begin{enum1}
	\item Choose a sample size, say $n=1000$. 
	\item Generate a treatment indicator $X$ to be 0/1 with 50-50 probability, X=runif(n) $\leq$ 0.5
	\item Let $Z$ be a covariate that is standard normal, e.g. Z=rnorm(n).
	\item Generate a binary endpoint that depends on $X$ and $Z$ according to the log-linear model as follows, Y = runif(n) $<$ exp(-1.0 + 0.5*X - 0.1*Z) 
	\item Use the following three approaches for modelling the dependence of $Y$ on $X$ and $Z$: (i) logistic regression, (ii) Poisson regression without robust standard errors, (iii) Poisson regression corrected for over or under dispersion.
	\item Discuss any substantive differences in the findings.
\end{enum1}

\section{Simulation}

\subsection{AUROC as Measure of Difference of Two Distributions}

The AUROC of a score that predicts an event equals the probability that a subject with the event will have a higher score than a person without the event. If the distribution of the scores in subjects with the event is normal with mean m1 and s1 and the distribution of scores in subjects without the event is normal with mean m0 and s0, then the following R line of code estimates the concordancy.
\begin{spacing}{1.0}
\begin{lstlisting}
mean( rnorm(n=n<-10^6, mean=m0, sd=s0) < rnorm(n=n, mean=m1, sd=s1) )
\end{lstlisting}
\end{spacing}

a. Create a table of Concordancy vs the following choices, m0=0,sd=1, m1 = 0.0, 0.25,0.5,0.75,1,1.5,2,3 and s1=1.

b. Suppose a score for the risk of an event is such that its distribution in those who will have the event is normal with mean m1 and standard deviation of s1, and its distribution in those who will not have the event is mean 0 and standard deviation 1. Simulate the score of 1000 events (cases)  and 1000 controls and plot the corresponding ROC curve for the following 4 scenarios (m1=0.5, s1=1), (m1=0.5, s1=2), (m1=2.0, s1=1), (m1=2.0, s1=2). To do this the following code could be helpful:
\begin{spacing}{1.0}
\begin{lstlisting}  
  Score1 <- rnorm(n=n, mean=m1,sd=s1)
  Score0 <- rnorm(n=n, mean=0,sd=1)
  Event <- rep(c(1,0), each=n)
  Score <- c(Score1, Score0)
  o <- roc(Event, Score)
  plot(o$specificities, o$sensitivities, type="line", 
\end{lstlisting}
\end{spacing}

 

\subsection{ADR vs APC}

The most recommended modality for colorectal cancer screening in the USA is colonoscopy. During a colonoscopy a clinician uses a camera at the end of a tube (colonoscope) to examine the colon. The colonoscope is also equipped with features to remove pre-cancerous lesions (polyps, adenomas). Colonoscopists vary in their ability to detect polyps. One measure of detection ability is the Adenoma Detection Rate (ADR). It is defined as the proportion of colonoscopies in which at least one adenoma is detected; like the proportion of games in which an athlete gets at least one point. An alternative metric is the APC (adenomas per colonoscopies); like the average number of points per game. Explain what the following simulation is doing and interpret the results.
\begin{spacing}{1.0}
\begin{lstlisting}  
R <- 1000
cor.ADR.true <- cor.APC.true <- R
n.endoscopists <- 200 # number of endoscopists in the cohort 
for (r in 1:R) {
  # number of patients each endoscopists scopes in a year
  n.pt.endoscopist <- ceiling(rgamma(n=n.endoscopists, shape=10, scale=30))
  N <- sum(n.pt.endoscopist) 
  ID.Endo <- rep(1:n.endoscopists, times=n.pt.endoscopist)
  true.endo.rate <- runif(n.endoscopists, min=0.35,max=0.99) # given a uniform distribution 
  long.true.endo.rate <- rep(true.endo.rate, times=n.pt.endoscopist)
  n.polyps <- rpois(n=N, lambda=0.6) # lambda is the average actual adenomas
  n.polyps.detected <- rbinom(n=N, size=n.polyps, prob=long.true.endo.rate)
  at.least.one <- n.polyps.detected>0
  ADR <- tapply(at.least.one, ID.Endo, mean)
  APC <- tapply(n.polyps.detected, ID.Endo, mean)
  cor.ADR.true[r] <- cor(ADR, true.endo.rate)
  cor.APC.true[r] <- cor(APC, true.endo.rate)
}
pairs(cbind(true.endo.rate, ADR, APC))
summary(cbind(cor.ADR.true, cor.APC.true))
\end{lstlisting}
\end{spacing}


\subsection{Ordinal Probit Regression}

Comment on what the following code is doing and illustrating. How you might interpret the coefficients in terms of a continuous latent variable?

\begin{spacing}{1.0}
\begin{lstlisting}

n <- 5000
Age <- 80*runif(n)
Sex <- ifelse(runif(n) < 0.5, "M", "F")
DM <- ifelse(runif(n) < 0.1, "Diabetes", "no Diabetes")
CKD <- ifelse(runif(n) < 0.05, "KidneyDisease", "No CKD")
coef <- c(Age=0.05, Male = 0.20, DM = 0.40, CKD = 0.60) 
Linear <- cbind(Age, Sex=="M", DM=="Diabetes", CKD == "KidneyDisease") %*% coef
Latent.Continuous <- Linear + rnorm(n)
cut.offs <- quantile(Latent.Continuous, c(0.60,0.85))
Discharge <- factor(cut(Latent.Continuous, c(-Inf, cut.offs, Inf), labels=c("Home","SNF","Dead")))

table(Discharge)

head(data.frame(Discharge, Age, Sex, DM, CKD))

library(MASS)
summary(o <- polr(Discharge ~ Age + Sex + DM + CKD, method="probit"))
o$coef - coef
o$coef / coef

\end{lstlisting}
\end{spacing}


\end{document}
